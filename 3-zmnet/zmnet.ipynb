{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# to retain original image color after style transfer\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_IMG = \"./images/style.png\"\n",
    "CONTENT_IMG = \"./images/content.jpg\"\n",
    "RESULT_IMG = \"./images/result.jpg\"\n",
    "\n",
    "STYLE_DIR = \"../pandorastyles/\"\n",
    "CONTENT_DIR = \"../contents/\"\n",
    "VGG_WEIGHT = \"../vgg16_weights.npz\"\n",
    "LOG_DIR = \"./logs\"\n",
    "MODEL_DIR = \"./models\"\n",
    "CKPT_DIR = \"./ckpts\"\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-3\n",
    "CONTENT_BATCH_SIZE = 8\n",
    "STYLE_BATCH_SIZE = 1\n",
    "LOG_ITER = 100\n",
    "SAMPLE_ITER = 100\n",
    "STYLE_SIZE = 256\n",
    "CONTENT_SIZE = 256\n",
    "\n",
    "CONTENT_LOSS_WEIGHT = 1\n",
    "STYLE_LOSS_WEIGHT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     6,
     13,
     20,
     23,
     30,
     37,
     40,
     47,
     54,
     61,
     64,
     71,
     78
    ]
   },
   "outputs": [],
   "source": [
    "def vgg16(x, weights):\n",
    "    # substract imagenet mean\n",
    "    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='imagenet_mean')\n",
    "    x = x - mean\n",
    "    \n",
    "    with tf.variable_scope(\"vgg16\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv1_1_W\"]), trainable=False, name='conv1_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv1_1_b\"]), trainable=False, name='conv1_1_b')\n",
    "            conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv1_1 = tf.nn.bias_add(conv1_1, biases)\n",
    "            conv1_1 = tf.nn.relu(conv1_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv1_2_W\"]), trainable=False, name='conv1_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv1_2_b\"]), trainable=False, name='conv1_2_b')\n",
    "            conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv1_2 = tf.nn.bias_add(conv1_2, biases)\n",
    "            conv1_2 = tf.nn.relu(conv1_2, name=scope)\n",
    "\n",
    "        pool1 = tf.nn.avg_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', \n",
    "                               name='pool1')\n",
    "\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv2_1_W\"]), trainable=False, name='conv2_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv2_1_b\"]), trainable=False, name='conv2_1_b')\n",
    "            conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv2_1 = tf.nn.bias_add(conv2_1, biases)\n",
    "            conv2_1 = tf.nn.relu(conv2_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv2_2_W\"]), trainable=False, name='conv2_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv2_2_b\"]), trainable=False, name='conv2_2_b')\n",
    "            conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv2_2 = tf.nn.bias_add(conv2_2, biases)\n",
    "            conv2_2 = tf.nn.relu(conv2_2, name=scope)\n",
    "\n",
    "        pool2 = tf.nn.avg_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_1_W\"]), trainable=False, name='conv3_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_1_b\"]), trainable=False, name='conv3_1_b')\n",
    "            conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_1 = tf.nn.bias_add(conv3_1, biases)\n",
    "            conv3_1 = tf.nn.relu(conv3_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_2_W\"]), trainable=False, name='conv3_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_2_b\"]), trainable=False, name='conv3_2_b')\n",
    "            conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_2 = tf.nn.bias_add(conv3_2, biases)\n",
    "            conv3_2 = tf.nn.relu(conv3_2, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_3_W\"]), trainable=False, name='conv3_3_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_3_b\"]), trainable=False, name='conv3_3_b')\n",
    "            conv3_3 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_3 = tf.nn.bias_add(conv3_3, biases)\n",
    "            conv3_3 = tf.nn.relu(conv3_3, name=scope)\n",
    "\n",
    "        pool3 = tf.nn.avg_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_1_W\"]), trainable=False, name='conv4_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_1_b\"]), trainable=False, name='conv4_1_b')\n",
    "            conv4_1 = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_1 = tf.nn.bias_add(conv4_1, biases)\n",
    "            conv4_1 = tf.nn.relu(conv4_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_2_W\"]), trainable=False, name='conv4_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_2_b\"]), trainable=False, name='conv4_2_b')\n",
    "            conv4_2 = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_2 = tf.nn.bias_add(conv4_2, biases)\n",
    "            conv4_2 = tf.nn.relu(conv4_2, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_3_W\"]), trainable=False, name='conv4_3_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_3_b\"]), trainable=False, name='conv4_3_b')\n",
    "            conv4_3 = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_3 = tf.nn.bias_add(conv4_3, biases)\n",
    "            conv4_3 = tf.nn.relu(conv4_3, name=scope)\n",
    "            \n",
    "    return conv1_2, conv2_2, conv3_3, conv4_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     18,
     27,
     33,
     63,
     70
    ]
   },
   "outputs": [],
   "source": [
    "def instance_norm(x, name, epsilon=1e-5, gamma=None, beta=None):\n",
    "    with tf.variable_scope(name):\n",
    "        if gamma is None: gamma = tf.get_variable(shape=x.shape[-1], name=\"gamma\")\n",
    "        if beta is None: beta = tf.get_variable(shape=x.shape[-1], name=\"beta\")\n",
    "        mean, var = tf.nn.moments(x, axes=[1,2], keep_dims=True)\n",
    "        x = tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon, name=\"norm\")\n",
    "    return x\n",
    "\n",
    "def conv(x, name, filters, kernel_size, strides, norm=instance_norm, act=tf.nn.relu, norm_gamma=None,\n",
    "         norm_beta=None):\n",
    "    padding = kernel_size//2\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.pad(x, paddings=[[0,0],[padding,padding],[padding,padding],[0,0]], mode=\"REFLECT\")\n",
    "        x = tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size, strides=strides, name=\"conv\")\n",
    "        if norm is not None: x = norm(x, name=\"norm\", gamma=norm_gamma, beta=norm_beta)\n",
    "        if act is not None: x = act(x, name=\"act\")\n",
    "    return x\n",
    "\n",
    "def residual_block(x, name, filters, kernel_size, norm_gamma1=None, norm_gamma2=None, norm_beta1=None, \n",
    "                   norm_beta2=None):\n",
    "    with tf.variable_scope(name):\n",
    "        residual = x\n",
    "        x = conv(x, \"conv1\", filters, kernel_size, strides=1, norm_gamma=norm_gamma1, norm_beta=norm_beta1)\n",
    "        x = conv(x, \"conv2\", filters, kernel_size, strides=1, act=None, norm_gamma=norm_gamma2, \n",
    "                 norm_beta=norm_beta2)\n",
    "    return x + residual\n",
    "\n",
    "def upsample(x, name, filters, kernel_size, strides, norm_gamma=None, norm_beta=None):\n",
    "    _, w, h, _ = x.shape.as_list()\n",
    "    x = tf.image.resize_images(x, size=[w * strides, h * strides])\n",
    "    x = conv(x, name, filters, kernel_size, strides=1, norm_gamma=norm_gamma, norm_beta=norm_beta)\n",
    "    return x\n",
    "\n",
    "def tnet(x, gammas, betas):   \n",
    "    with tf.variable_scope(\"tnet\", reuse=tf.AUTO_REUSE):\n",
    "        conv1 = conv(x, \"conv1\", filters=32, kernel_size=9, strides=1, norm_gamma=gammas[\"conv1\"],\n",
    "                     norm_beta=betas[\"conv1\"])\n",
    "        conv2 = conv(conv1, \"conv2\", filters=64, kernel_size=3, strides=2, norm_gamma=gammas[\"conv2\"],\n",
    "                     norm_beta=betas[\"conv2\"])\n",
    "        conv3 = conv(conv2, \"conv3\", filters=128, kernel_size=3, strides=2, norm_gamma=gammas[\"conv3\"],\n",
    "                     norm_beta=betas[\"conv3\"])\n",
    "        res1 = residual_block(conv3, \"res1\", filters=128, kernel_size=3, norm_gamma1=gammas[\"res1_1\"],\n",
    "                              norm_gamma2=gammas[\"res1_2\"], norm_beta1=betas[\"res1_1\"], \n",
    "                              norm_beta2=betas[\"res1_2\"])\n",
    "        res2 = residual_block(res1, \"res2\", filters=128, kernel_size=3, norm_gamma1=gammas[\"res2_1\"],\n",
    "                              norm_gamma2=gammas[\"res2_2\"], norm_beta1=betas[\"res2_1\"], \n",
    "                              norm_beta2=betas[\"res2_2\"])\n",
    "        res3 = residual_block(res2, \"res3\", filters=128, kernel_size=3, norm_gamma1=gammas[\"res3_1\"],\n",
    "                              norm_gamma2=gammas[\"res3_2\"], norm_beta1=betas[\"res3_1\"], \n",
    "                              norm_beta2=betas[\"res3_2\"])\n",
    "        res4 = residual_block(res3, \"res4\", filters=128, kernel_size=3, norm_gamma1=gammas[\"res4_1\"],\n",
    "                              norm_gamma2=gammas[\"res4_2\"], norm_beta1=betas[\"res4_1\"], \n",
    "                              norm_beta2=betas[\"res4_2\"])\n",
    "        res5 = residual_block(res4, \"res5\", filters=128, kernel_size=3, norm_gamma1=gammas[\"res5_1\"],\n",
    "                              norm_gamma2=gammas[\"res5_2\"], norm_beta1=betas[\"res5_1\"], \n",
    "                              norm_beta2=betas[\"res5_2\"])\n",
    "        up1 = upsample(res5, \"up1\", filters=64, kernel_size=3, strides=2, norm_gamma=gammas[\"up1\"],\n",
    "                       norm_beta=betas[\"up1\"])\n",
    "        up2 = upsample(up1, \"up2\", filters=32, kernel_size=3, strides=2, norm_gamma=gammas[\"up2\"],\n",
    "                       norm_beta=betas[\"up2\"])\n",
    "        conv4 = conv(up2, \"conv4\", filters=3, kernel_size=9, strides=1, norm=None, act=None)\n",
    "    return tf.clip_by_value(conv4, 0., 255.)\n",
    "\n",
    "def pnet_residual_block(x, name, filters, kernel_size):\n",
    "    with tf.variable_scope(name):\n",
    "        residual = x\n",
    "        conv1 = conv(x, \"conv1\", filters, kernel_size, strides=1)\n",
    "        conv2 = conv(conv1, \"conv2\", filters, kernel_size, strides=1)\n",
    "    return conv2 + residual, conv1, conv2\n",
    "\n",
    "def pnet_fc(x, name):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.reshape(x, shape=[x.shape[1] * x.shape[2], x.shape[3]])\n",
    "        w = tf.get_variable(shape=[1, x.shape[0]], name=\"w\")\n",
    "        b = tf.get_variable(shape=[1, x.shape[1]], name=\"b\")\n",
    "        fc = tf.squeeze(w @ x + b)\n",
    "    return fc\n",
    "\n",
    "def pnet(x): \n",
    "    gammas, betas = {}, {}\n",
    "    with tf.variable_scope(\"pnet\", reuse=tf.AUTO_REUSE):\n",
    "        conv1 = conv(x, \"conv1\", filters=32, kernel_size=9, strides=1)\n",
    "        gammas[\"conv1\"], betas[\"conv1\"] = pnet_fc(conv1, \"fc_gamma_conv1\"), pnet_fc(conv1, \"fc_beta_conv1\")\n",
    "        \n",
    "        conv2 = conv(conv1, \"conv2\", filters=64, kernel_size=3, strides=2)\n",
    "        gammas[\"conv2\"], betas[\"conv2\"] = pnet_fc(conv2, \"fc_gamma_conv2\"), pnet_fc(conv2, \"fc_beta_conv2\")\n",
    "        \n",
    "        conv3 = conv(conv2, \"conv3\", filters=128, kernel_size=3, strides=2)\n",
    "        gammas[\"conv3\"], betas[\"conv3\"] = pnet_fc(conv3, \"fc_gamma_conv3\"), pnet_fc(conv3, \"fc_beta_conv3\")\n",
    "        \n",
    "        res1, res1_1, res1_2 = pnet_residual_block(conv3, \"res1\", filters=128, kernel_size=3)\n",
    "        gammas[\"res1_1\"], betas[\"res1_1\"] = pnet_fc(res1_1, \"fc_gamma_res1_1\"), pnet_fc(res1_1, \"fc_beta_res1_1\")\n",
    "        gammas[\"res1_2\"], betas[\"res1_2\"] = pnet_fc(res1_2, \"fc_gamma_res1_2\"), pnet_fc(res1_2, \"fc_beta_res1_2\")\n",
    "        \n",
    "        res2, res2_1, res2_2 = pnet_residual_block(res1, \"res2\", filters=128, kernel_size=3)\n",
    "        gammas[\"res2_1\"], betas[\"res2_1\"] = pnet_fc(res2_1, \"fc_gamma_res2_1\"), pnet_fc(res2_1, \"fc_beta_res2_1\")\n",
    "        gammas[\"res2_2\"], betas[\"res2_2\"] = pnet_fc(res2_2, \"fc_gamma_res2_2\"), pnet_fc(res2_2, \"fc_beta_res2_2\")\n",
    "        \n",
    "        res3, res3_1, res3_2 = pnet_residual_block(res2, \"res3\", filters=128, kernel_size=3)\n",
    "        gammas[\"res3_1\"], betas[\"res3_1\"] = pnet_fc(res3_1, \"fc_gamma_res3_1\"), pnet_fc(res3_1, \"fc_beta_res3_1\")\n",
    "        gammas[\"res3_2\"], betas[\"res3_2\"] = pnet_fc(res3_2, \"fc_gamma_res3_2\"), pnet_fc(res3_2, \"fc_beta_res3_2\")\n",
    "        \n",
    "        res4, res4_1, res4_2 = pnet_residual_block(res3, \"res4\", filters=128, kernel_size=3)\n",
    "        gammas[\"res4_1\"], betas[\"res4_1\"] = pnet_fc(res4_1, \"fc_gamma_res4_1\"), pnet_fc(res4_1, \"fc_beta_res4_1\")\n",
    "        gammas[\"res4_2\"], betas[\"res4_2\"] = pnet_fc(res4_2, \"fc_gamma_res4_2\"), pnet_fc(res4_2, \"fc_beta_res4_2\")\n",
    "        \n",
    "        res5, res5_1, res5_2 = pnet_residual_block(res4, \"res5\", filters=128, kernel_size=3)\n",
    "        gammas[\"res5_1\"], betas[\"res5_1\"] = pnet_fc(res5_1, \"fc_gamma_res5_1\"), pnet_fc(res5_1, \"fc_beta_res5_1\")\n",
    "        gammas[\"res5_2\"], betas[\"res5_2\"] = pnet_fc(res5_2, \"fc_gamma_res5_2\"), pnet_fc(res5_2, \"fc_beta_res5_2\")\n",
    "        \n",
    "        up1 = upsample(res5, \"up1\", filters=64, kernel_size=3, strides=2)\n",
    "        gammas[\"up1\"], betas[\"up1\"] = pnet_fc(up1, \"fc_gamma_up1\"), pnet_fc(up1, \"fc_beta_up1\")\n",
    "        \n",
    "        up2 = upsample(up1, \"up2\", filters=32, kernel_size=3, strides=2)\n",
    "        gammas[\"up2\"], betas[\"up2\"] = pnet_fc(up2, \"fc_gamma_up2\"), pnet_fc(up2, \"fc_beta_up2\")\n",
    "    return gammas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    batch_size, w, h, ch = x.shape.as_list()\n",
    "    x = tf.reshape(x, [batch_size, w * h, ch])\n",
    "    return tf.matmul(x, x, transpose_a=True) / (ch * w * h)\n",
    "\n",
    "def loss_fun(target_style_features, target_content_features, transferred_features, \n",
    "             style_loss_weight=STYLE_LOSS_WEIGHT, content_loss_weight=CONTENT_LOSS_WEIGHT):\n",
    "    # using relu2_2 as content features\n",
    "    content_loss = tf.reduce_mean(tf.subtract(transferred_features[1], target_content_features[1]) ** 2, \n",
    "                                  [1, 2, 3])  \n",
    "\n",
    "    style_loss = 0\n",
    "    for i in range(len(transferred_features)):\n",
    "        gram_target = gram_matrix(target_style_features[i])\n",
    "        gram_transferred = gram_matrix(transferred_features[i])\n",
    "        style_loss += tf.reduce_mean(tf.subtract(gram_target, gram_transferred) ** 2, [1, 2])  \n",
    "        \n",
    "    return tf.reduce_mean(content_loss_weight * content_loss + style_loss_weight * style_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     4
    ]
   },
   "outputs": [],
   "source": [
    "iterator = tf.keras.preprocessing.image.DirectoryIterator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "content_iter = iterator(directory=CONTENT_DIR, batch_size=CONTENT_BATCH_SIZE, \n",
    "                        target_size=(CONTENT_SIZE,CONTENT_SIZE), image_data_generator=datagen, shuffle=True)\n",
    "style_iter = iterator(directory=STYLE_DIR, batch_size=STYLE_BATCH_SIZE,\n",
    "                      target_size=(STYLE_SIZE,STYLE_SIZE), image_data_generator=datagen, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13,
     39
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(CKPT_DIR):\n",
    "    shutil.rmtree(CKPT_DIR)\n",
    "    \n",
    "vgg_weights = np.load(VGG_WEIGHT)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "total_iteration = NUM_EPOCHS * content_iter.n // CONTENT_BATCH_SIZE\n",
    "\n",
    "training_graph = tf.Graph()\n",
    "\n",
    "with training_graph.as_default() as g, tf.Session(config=config, graph=training_graph) as sess:    \n",
    "    style = tf.placeholder(name=\"style\", dtype=tf.float32, \n",
    "                           shape=[STYLE_BATCH_SIZE,STYLE_SIZE,STYLE_SIZE,3])\n",
    "    content = tf.placeholder(name=\"content\", dtype=tf.float32, \n",
    "                             shape=[CONTENT_BATCH_SIZE,CONTENT_SIZE,CONTENT_SIZE,3])\n",
    "\n",
    "    target_style_features = vgg16(style, vgg_weights)\n",
    "    target_content_features = vgg16(content, vgg_weights)\n",
    "    gammas, betas = pnet(style)\n",
    "    transferred = tnet(content, gammas, betas)\n",
    "    transferred_features = vgg16(transferred, vgg_weights)\n",
    "    loss = loss_fun(target_style_features, target_content_features, transferred_features)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    summary = tf.summary.FileWriter(graph=g, logdir=LOG_DIR)\n",
    "    style_summary = tf.summary.image(\"style\", style)\n",
    "    content_summary = tf.summary.image(\"content\", content)\n",
    "    transferred_summary = tf.summary.image(\"transferred\", transferred)\n",
    "    image_summary = tf.summary.merge([style_summary, content_summary, transferred_summary])\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
    "\n",
    "    start = time.time()\n",
    "    it = 0\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        content_iter.reset()\n",
    "        style_iter.reset()\n",
    "        for c, _ in content_iter:\n",
    "            it += 1\n",
    "            \n",
    "            if c.shape[0] < CONTENT_BATCH_SIZE:\n",
    "                break\n",
    "                 \n",
    "            try:\n",
    "                s, _ = style_iter.next()\n",
    "            except StopIteration:\n",
    "                style_iter.reset()\n",
    "                s, _ = style_iter.next()\n",
    "                    \n",
    "            _, cur_loss, cur_loss_summary, cur_image_summary = sess.run([train_op, loss, loss_summary, \n",
    "                                                                         image_summary], \n",
    "                                                                        feed_dict={style: s, content: c})\n",
    "            summary.add_summary(cur_loss_summary, it)\n",
    "\n",
    "            if it % LOG_ITER == 0 or it == total_iteration:\n",
    "                print(\"Iteration: [{it}/{num_iter}], loss: {loss}\".format(it=it, num_iter=total_iteration,\n",
    "                                                                          loss=cur_loss))\n",
    "                \n",
    "            if it % SAMPLE_ITER == 0 or it == total_iteration:\n",
    "                summary.add_summary(cur_image_summary, it)\n",
    "                \n",
    "            summary.flush()\n",
    "            \n",
    "        ckpt_path = saver.save(sess, save_path=os.path.join(CKPT_DIR, \"ckpt\"), write_meta_graph=False, \n",
    "                               global_step=it)\n",
    "        print(\"Checkpoint saved as: {ckpt_path}\".format(ckpt_path=ckpt_path))\n",
    "        \n",
    "end = time.time()\n",
    "print(\"Finished {num_iter} in {time} seconds\".format(num_iter=total_iteration, time=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_DIR):\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "eval_graph = tf.Graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "exporter = tf.saved_model.builder.SavedModelBuilder(MODEL_DIR)\n",
    "latest_ckpt = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "\n",
    "with eval_graph.as_default() as g, tf.Session(config=config, graph=eval_graph) as sess:    \n",
    "    style = tf.placeholder(name=\"style\", dtype=tf.float32, shape=[1,STYLE_SIZE,STYLE_SIZE,3])\n",
    "    inputs = tf.placeholder(name=\"inputs\", dtype=tf.float32, shape=[None,CONTENT_SIZE,CONTENT_SIZE,3])\n",
    "    gammas, betas = pnet(style)\n",
    "    outputs = tf.identity(tnet(inputs, gammas, betas), name=\"outputs\")\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, latest_ckpt)\n",
    "    \n",
    "    exporter.add_meta_graph_and_variables(\n",
    "        sess, \n",
    "        tags=[tf.saved_model.tag_constants.SERVING], \n",
    "        signature_def_map={\n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "            tf.saved_model.signature_def_utils.predict_signature_def(inputs={\"inputs\": inputs,\n",
    "                                                                             \"style\": style}, \n",
    "                                                                     outputs={\"outputs\": outputs})\n",
    "        })\n",
    "    exporter.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "content_image = tf.keras.preprocessing.image.img_to_array(img=tf.keras.preprocessing.image.load_img(CONTENT_IMG, target_size=(CONTENT_SIZE,CONTENT_SIZE)))\n",
    "style_image = tf.keras.preprocessing.image.img_to_array(img=\n",
    "                                                        tf.keras.preprocessing.image.load_img(\"../styles/0/the_scream.jpg\", target_size=(STYLE_SIZE,STYLE_SIZE)))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "eval_graph = tf.Graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with eval_graph.as_default() as g, tf.Session(config=config, graph=eval_graph) as sess:  \n",
    "    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], MODEL_DIR)\n",
    "    inputs = g.get_tensor_by_name(\"inputs:0\")\n",
    "    style = g.get_tensor_by_name(\"style:0\")\n",
    "    outputs = g.get_tensor_by_name(\"outputs:0\")    \n",
    "    c, s = sess.run([tf.expand_dims(content_image, axis=0), tf.expand_dims(style_image, axis=0)])\n",
    "    start = time.time()\n",
    "    result = sess.run(tf.squeeze(outputs), feed_dict={inputs: c, style: s})\n",
    "    end = time.time()\n",
    "    print(\"Inference time: {time} seconds\".format(time=end-start))\n",
    "    \n",
    "# retain original image color\n",
    "# def use_original_color(original, result):\n",
    "#     result_hsv = rgb_to_hsv(result)\n",
    "#     orig_hsv = rgb_to_hsv(original)\n",
    "#     oh, os, ov = np.split(orig_hsv, axis=-1, indices_or_sections=3)\n",
    "#     rh, rs, rv = np.split(result_hsv, axis=-1, indices_or_sections=3)\n",
    "#     return hsv_to_rgb(np.concatenate([oh, os, rv], axis=-1))\n",
    "\n",
    "# final_result = use_original_color(content_image.reshape((CONTENT_SIZE, CONTENT_SIZE, 3)), result)\n",
    "final_result = result\n",
    "plt.imshow(final_result / 255.)    \n",
    "plt.show()\n",
    "\n",
    "result_image = tf.keras.preprocessing.image.array_to_img(final_result)\n",
    "result_image.save(RESULT_IMG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
