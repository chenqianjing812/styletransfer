{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikatok/miniconda3/envs/deeplearning/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# to retain original image color after style transfer\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_IMG = \"./images/style.png\"\n",
    "CONTENT_IMG = \"./images/content.jpg\"\n",
    "RESULT_IMG = \"./images/result.jpg\"\n",
    "\n",
    "STYLE_DIR = \"../styles/\"\n",
    "CONTENT_DIR = \"../contents/\"\n",
    "VGG_WEIGHT = \"../vgg16_weights.npz\"\n",
    "LOG_DIR = \"./logs\"\n",
    "MODEL_DIR = \"./models\"\n",
    "CKPT_DIR = \"./ckpts\"\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "CONTENT_BATCH_SIZE = 8\n",
    "STYLE_BATCH_SIZE = 1\n",
    "LOG_ITER = 100\n",
    "SAMPLE_ITER = 100\n",
    "STYLE_SIZE = 256\n",
    "CONTENT_SIZE = 256\n",
    "\n",
    "CONTENT_LOSS_WEIGHT = 1\n",
    "STYLE_LOSS_WEIGHT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def vgg16(x, weights):\n",
    "    # substract imagenet mean\n",
    "    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='imagenet_mean')\n",
    "    x = x - mean\n",
    "    \n",
    "    with tf.variable_scope(\"vgg16\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv1_1_W\"]), trainable=False, name='conv1_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv1_1_b\"]), trainable=False, name='conv1_1_b')\n",
    "            conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv1_1 = tf.nn.bias_add(conv1_1, biases)\n",
    "            conv1_1 = tf.nn.relu(conv1_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv1_2_W\"]), trainable=False, name='conv1_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv1_2_b\"]), trainable=False, name='conv1_2_b')\n",
    "            conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv1_2 = tf.nn.bias_add(conv1_2, biases)\n",
    "            conv1_2 = tf.nn.relu(conv1_2, name=scope)\n",
    "\n",
    "        pool1 = tf.nn.avg_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', \n",
    "                               name='pool1')\n",
    "\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv2_1_W\"]), trainable=False, name='conv2_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv2_1_b\"]), trainable=False, name='conv2_1_b')\n",
    "            conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv2_1 = tf.nn.bias_add(conv2_1, biases)\n",
    "            conv2_1 = tf.nn.relu(conv2_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv2_2_W\"]), trainable=False, name='conv2_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv2_2_b\"]), trainable=False, name='conv2_2_b')\n",
    "            conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv2_2 = tf.nn.bias_add(conv2_2, biases)\n",
    "            conv2_2 = tf.nn.relu(conv2_2, name=scope)\n",
    "\n",
    "        pool2 = tf.nn.avg_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_1_W\"]), trainable=False, name='conv3_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_1_b\"]), trainable=False, name='conv3_1_b')\n",
    "            conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_1 = tf.nn.bias_add(conv3_1, biases)\n",
    "            conv3_1 = tf.nn.relu(conv3_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_2_W\"]), trainable=False, name='conv3_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_2_b\"]), trainable=False, name='conv3_2_b')\n",
    "            conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_2 = tf.nn.bias_add(conv3_2, biases)\n",
    "            conv3_2 = tf.nn.relu(conv3_2, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_3_W\"]), trainable=False, name='conv3_3_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_3_b\"]), trainable=False, name='conv3_3_b')\n",
    "            conv3_3 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_3 = tf.nn.bias_add(conv3_3, biases)\n",
    "            conv3_3 = tf.nn.relu(conv3_3, name=scope)\n",
    "\n",
    "        pool3 = tf.nn.avg_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_1_W\"]), trainable=False, name='conv4_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_1_b\"]), trainable=False, name='conv4_1_b')\n",
    "            conv4_1 = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_1 = tf.nn.bias_add(conv4_1, biases)\n",
    "            conv4_1 = tf.nn.relu(conv4_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_2_W\"]), trainable=False, name='conv4_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_2_b\"]), trainable=False, name='conv4_2_b')\n",
    "            conv4_2 = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_2 = tf.nn.bias_add(conv4_2, biases)\n",
    "            conv4_2 = tf.nn.relu(conv4_2, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_3_W\"]), trainable=False, name='conv4_3_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_3_b\"]), trainable=False, name='conv4_3_b')\n",
    "            conv4_3 = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_3 = tf.nn.bias_add(conv4_3, biases)\n",
    "            conv4_3 = tf.nn.relu(conv4_3, name=scope)\n",
    "            \n",
    "    return conv1_2, conv2_2, conv3_3, conv4_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     8,
     17,
     24,
     30,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def instance_norm(x, name, epsilon=1e-5):\n",
    "    with tf.variable_scope(name):\n",
    "        gamma = tf.get_variable(shape=[x.shape[-1]], name=\"gamma\")\n",
    "        beta = tf.get_variable(shape=[x.shape[-1]], name=\"beta\")\n",
    "        mean, var = tf.nn.moments(x, axes=[1,2], keep_dims=True)\n",
    "        x = tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon, name=\"norm\",)\n",
    "    return x\n",
    "\n",
    "def conv(x, name, filters, kernel_size, strides, norm=instance_norm, act=tf.nn.relu):\n",
    "    padding = kernel_size//2\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.pad(x, paddings=[[0,0],[padding,padding],[padding,padding],[0,0]], mode=\"REFLECT\")\n",
    "        x = tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size, strides=strides, name=\"conv\")\n",
    "        if norm is not None: x = norm(x, name=\"norm\")\n",
    "        if act is not None: x = act(x, name=\"act\")\n",
    "    return x\n",
    "\n",
    "def residual_block(x, name, filters, kernel_size):\n",
    "    with tf.variable_scope(name):\n",
    "        residual = x\n",
    "        x = conv(x, \"conv1\", filters, kernel_size, strides=1)\n",
    "        x = conv(x, \"conv2\", filters, kernel_size, strides=1, act=None)\n",
    "    return x + residual\n",
    "\n",
    "def upsample(x, name, filters, kernel_size, strides):\n",
    "    _, w, h, _ = x.shape.as_list()\n",
    "    x = tf.image.resize_images(x, size=[w * strides, h * strides])\n",
    "    x = conv(x, name, filters, kernel_size, strides=1)\n",
    "    return x\n",
    "\n",
    "def gram_matrix(x):\n",
    "    batch_size, w, h, ch = x.shape.as_list()\n",
    "    x = tf.reshape(x, [batch_size, w * h, ch])\n",
    "    return tf.matmul(x, x, transpose_a=True) / (ch * w * h)\n",
    "       \n",
    "def inspiration(x, name, style_gram):\n",
    "    with tf.variable_scope(name):\n",
    "        bs, w, h, ch = x.shape.as_list()\n",
    "        weight = tf.get_variable(shape=[1,ch,ch], name=\"w\")\n",
    "        x = tf.reshape(x, shape=[bs, w * h, ch])\n",
    "        x = x @ tf.tile(weight @ style_gram, [bs,1,1])\n",
    "        x = tf.reshape(x, shape=[bs, w, h, ch])\n",
    "    return x\n",
    "        \n",
    "def tnet(x, style_gram=None):    \n",
    "    with tf.variable_scope(\"tnet\", reuse=tf.AUTO_REUSE):\n",
    "        conv1 = conv(x, \"conv1\", filters=32, kernel_size=9, strides=1)\n",
    "        conv2 = conv(conv1, \"conv2\", filters=64, kernel_size=3, strides=2)\n",
    "        conv3 = conv(conv2, \"conv3\", filters=128, kernel_size=3, strides=2)\n",
    "        if style_gram is None:\n",
    "            return gram_matrix(conv3)\n",
    "        ins = inspiration(conv3, \"inspiration\", style_gram)\n",
    "        res1 = residual_block(ins, \"res1\", filters=128, kernel_size=3)\n",
    "        res2 = residual_block(res1, \"res2\", filters=128, kernel_size=3)\n",
    "        res3 = residual_block(res2, \"res3\", filters=128, kernel_size=3)\n",
    "        res4 = residual_block(res3, \"res4\", filters=128, kernel_size=3)\n",
    "        res5 = residual_block(res4, \"res5\", filters=128, kernel_size=3)\n",
    "        up1 = upsample(res5, \"up1\", filters=64, kernel_size=3, strides=2)\n",
    "        up2 = upsample(up1, \"up2\", filters=32, kernel_size=3, strides=2)\n",
    "        conv4 = conv(up2, \"conv4\", filters=3, kernel_size=9, strides=1, norm=None, act=None)\n",
    "    return tf.clip_by_value(conv4, 0., 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loss_fun(target_style_features, target_content_features, transferred_features, \n",
    "             style_loss_weight=STYLE_LOSS_WEIGHT, content_loss_weight=CONTENT_LOSS_WEIGHT):\n",
    "    # using relu2_2 as content features\n",
    "    content_loss = tf.reduce_mean(tf.subtract(transferred_features[1], target_content_features[1]) ** 2, \n",
    "                                  [1, 2, 3])  \n",
    "\n",
    "    style_loss = 0\n",
    "    for i in range(len(transferred_features)):\n",
    "        gram_target = gram_matrix(target_style_features[i])\n",
    "        gram_transferred = gram_matrix(transferred_features[i])\n",
    "        style_loss += tf.reduce_mean(tf.subtract(gram_target, gram_transferred) ** 2, [1, 2])  \n",
    "        \n",
    "    return tf.reduce_mean(content_loss_weight * content_loss + style_loss_weight * style_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     2,
     4
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82783 images belonging to 1 classes.\n",
      "Found 21 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "iterator = tf.keras.preprocessing.image.DirectoryIterator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "content_iter = iterator(directory=CONTENT_DIR, batch_size=CONTENT_BATCH_SIZE, \n",
    "                        target_size=(CONTENT_SIZE,CONTENT_SIZE), image_data_generator=datagen, shuffle=True)\n",
    "style_iter = iterator(directory=STYLE_DIR, batch_size=STYLE_BATCH_SIZE,\n",
    "                      target_size=(STYLE_SIZE,STYLE_SIZE), image_data_generator=datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: [100/41391], loss: 2642383.0\n",
      "Iteration: [200/41391], loss: 2487648.0\n",
      "Iteration: [300/41391], loss: 1457973.25\n",
      "Iteration: [400/41391], loss: 3961371.75\n",
      "Iteration: [500/41391], loss: 835201.0625\n",
      "Iteration: [600/41391], loss: 4101747.5\n",
      "Iteration: [700/41391], loss: 2336969.5\n",
      "Iteration: [800/41391], loss: 1761990.25\n",
      "Iteration: [900/41391], loss: 3171068.5\n",
      "Iteration: [1000/41391], loss: 4293790.0\n",
      "Iteration: [1100/41391], loss: 2100701.0\n",
      "Iteration: [1200/41391], loss: 1088736.0\n",
      "Iteration: [1300/41391], loss: 3315414.0\n",
      "Iteration: [1400/41391], loss: 3658624.0\n",
      "Iteration: [1500/41391], loss: 2509923.0\n",
      "Iteration: [1600/41391], loss: 610421.75\n",
      "Iteration: [1700/41391], loss: 2015999.0\n",
      "Iteration: [1800/41391], loss: 3697628.0\n",
      "Iteration: [1900/41391], loss: 1116186.75\n",
      "Iteration: [2000/41391], loss: 1614273.875\n",
      "Iteration: [2100/41391], loss: 769563.125\n",
      "Iteration: [2200/41391], loss: 1558573.25\n",
      "Iteration: [2300/41391], loss: 2359229.5\n",
      "Iteration: [2400/41391], loss: 2140641.0\n",
      "Iteration: [2500/41391], loss: 2078145.125\n",
      "Iteration: [2600/41391], loss: 1364442.0\n",
      "Iteration: [2700/41391], loss: 652061.375\n",
      "Iteration: [2800/41391], loss: 795492.375\n",
      "Iteration: [2900/41391], loss: 1876938.25\n",
      "Iteration: [3000/41391], loss: 463108.625\n",
      "Iteration: [3100/41391], loss: 684059.5\n",
      "Iteration: [3200/41391], loss: 1046592.875\n",
      "Iteration: [3300/41391], loss: 1027377.1875\n",
      "Iteration: [3400/41391], loss: 397173.40625\n",
      "Iteration: [3500/41391], loss: 238282.6875\n",
      "Iteration: [3600/41391], loss: 795235.4375\n",
      "Iteration: [3700/41391], loss: 547575.25\n",
      "Iteration: [3800/41391], loss: 573469.0\n",
      "Iteration: [3900/41391], loss: 349050.875\n",
      "Iteration: [4000/41391], loss: 1318683.5\n",
      "Iteration: [4100/41391], loss: 299498.6875\n",
      "Iteration: [4200/41391], loss: 816206.0\n",
      "Iteration: [4300/41391], loss: 692834.125\n",
      "Iteration: [4400/41391], loss: 800593.25\n",
      "Iteration: [4500/41391], loss: 500955.5\n",
      "Iteration: [4600/41391], loss: 824277.375\n",
      "Iteration: [4700/41391], loss: 427350.0\n",
      "Iteration: [4800/41391], loss: 270795.5625\n",
      "Iteration: [4900/41391], loss: 764732.75\n",
      "Iteration: [5000/41391], loss: 688499.125\n",
      "Iteration: [5100/41391], loss: 529732.375\n",
      "Iteration: [5200/41391], loss: 273486.96875\n",
      "Iteration: [5300/41391], loss: 364195.8125\n",
      "Iteration: [5400/41391], loss: 307547.0\n",
      "Iteration: [5500/41391], loss: 281094.71875\n",
      "Iteration: [5600/41391], loss: 354164.25\n",
      "Iteration: [5700/41391], loss: 376932.03125\n",
      "Iteration: [5800/41391], loss: 1882602.25\n",
      "Iteration: [5900/41391], loss: 948122.125\n",
      "Iteration: [6000/41391], loss: 1522767.5\n",
      "Iteration: [6100/41391], loss: 734089.3125\n",
      "Iteration: [6200/41391], loss: 771601.3125\n",
      "Iteration: [6300/41391], loss: 485265.5\n",
      "Iteration: [6400/41391], loss: 394285.25\n",
      "Iteration: [6500/41391], loss: 612188.8125\n",
      "Iteration: [6600/41391], loss: 1877070.25\n",
      "Iteration: [6700/41391], loss: 195167.078125\n",
      "Iteration: [6800/41391], loss: 498988.8125\n",
      "Iteration: [6900/41391], loss: 376960.0\n",
      "Iteration: [7000/41391], loss: 281050.3125\n",
      "Iteration: [7100/41391], loss: 449335.5\n",
      "Iteration: [7200/41391], loss: 418954.125\n",
      "Iteration: [7300/41391], loss: 339906.5\n",
      "Iteration: [7400/41391], loss: 264469.25\n",
      "Iteration: [7500/41391], loss: 295088.90625\n",
      "Iteration: [7600/41391], loss: 261748.890625\n",
      "Iteration: [7700/41391], loss: 598953.6875\n",
      "Iteration: [7800/41391], loss: 760555.6875\n",
      "Iteration: [7900/41391], loss: 516065.0\n",
      "Iteration: [8000/41391], loss: 333893.125\n",
      "Iteration: [8100/41391], loss: 202425.390625\n",
      "Iteration: [8200/41391], loss: 349463.5\n",
      "Iteration: [8300/41391], loss: 426526.46875\n",
      "Iteration: [8400/41391], loss: 275022.0625\n",
      "Iteration: [8500/41391], loss: 194900.328125\n",
      "Iteration: [8600/41391], loss: 480261.03125\n",
      "Iteration: [8700/41391], loss: 290132.3125\n",
      "Iteration: [8800/41391], loss: 453142.125\n",
      "Iteration: [8900/41391], loss: 259911.5\n",
      "Iteration: [9000/41391], loss: 462596.46875\n",
      "Iteration: [9100/41391], loss: 899906.375\n",
      "Iteration: [9200/41391], loss: 212259.6875\n",
      "Iteration: [9300/41391], loss: 284414.625\n",
      "Iteration: [9400/41391], loss: 414839.25\n",
      "Iteration: [9500/41391], loss: 178351.421875\n",
      "Iteration: [9600/41391], loss: 339509.5\n",
      "Iteration: [9700/41391], loss: 390688.25\n",
      "Iteration: [9800/41391], loss: 226886.28125\n",
      "Iteration: [9900/41391], loss: 246647.34375\n",
      "Iteration: [10000/41391], loss: 185654.5\n",
      "Iteration: [10100/41391], loss: 109005.71875\n",
      "Iteration: [10200/41391], loss: 311514.875\n",
      "Iteration: [10300/41391], loss: 259603.3125\n",
      "Checkpoint saved as: ./ckpts/ckpt-10348\n",
      "Iteration: [10400/41391], loss: 724954.375\n",
      "Iteration: [10500/41391], loss: 808774.375\n",
      "Iteration: [10600/41391], loss: 415026.03125\n",
      "Iteration: [10700/41391], loss: 468872.4375\n",
      "Iteration: [10800/41391], loss: 280256.125\n",
      "Iteration: [10900/41391], loss: 257380.65625\n",
      "Iteration: [11000/41391], loss: 168368.78125\n",
      "Iteration: [11100/41391], loss: 322400.5\n",
      "Iteration: [11200/41391], loss: 146556.703125\n",
      "Iteration: [11300/41391], loss: 197268.15625\n",
      "Iteration: [11400/41391], loss: 367117.40625\n",
      "Iteration: [11500/41391], loss: 231028.71875\n",
      "Iteration: [11600/41391], loss: 145976.71875\n",
      "Iteration: [11700/41391], loss: 501152.0625\n",
      "Iteration: [11800/41391], loss: 279580.75\n",
      "Iteration: [11900/41391], loss: 178101.53125\n",
      "Iteration: [12000/41391], loss: 392313.8125\n",
      "Iteration: [12100/41391], loss: 280093.6875\n",
      "Iteration: [12200/41391], loss: 367149.3125\n",
      "Iteration: [12300/41391], loss: 199703.625\n",
      "Iteration: [12400/41391], loss: 212369.453125\n",
      "Iteration: [12500/41391], loss: 311394.9375\n",
      "Iteration: [12600/41391], loss: 439170.9375\n",
      "Iteration: [12700/41391], loss: 1198310.5\n",
      "Iteration: [12800/41391], loss: 745358.8125\n",
      "Iteration: [12900/41391], loss: 366952.125\n",
      "Iteration: [13000/41391], loss: 259597.890625\n",
      "Iteration: [13100/41391], loss: 264495.28125\n",
      "Iteration: [13200/41391], loss: 127274.640625\n",
      "Iteration: [13300/41391], loss: 341522.8125\n",
      "Iteration: [13400/41391], loss: 186788.03125\n",
      "Iteration: [13500/41391], loss: 148872.0\n",
      "Iteration: [13600/41391], loss: 303704.0\n",
      "Iteration: [13700/41391], loss: 409117.8125\n",
      "Iteration: [13800/41391], loss: 163358.46875\n",
      "Iteration: [13900/41391], loss: 262128.859375\n",
      "Iteration: [14000/41391], loss: 169769.90625\n",
      "Iteration: [14100/41391], loss: 362737.3125\n",
      "Iteration: [14200/41391], loss: 104456.3671875\n",
      "Iteration: [14300/41391], loss: 291106.3125\n",
      "Iteration: [14400/41391], loss: 231627.5\n",
      "Iteration: [14500/41391], loss: 348912.625\n",
      "Iteration: [14600/41391], loss: 289273.9375\n",
      "Iteration: [14700/41391], loss: 329341.71875\n",
      "Iteration: [14800/41391], loss: 339965.96875\n",
      "Iteration: [14900/41391], loss: 169703.5\n",
      "Iteration: [15000/41391], loss: 299382.28125\n",
      "Iteration: [15100/41391], loss: 390621.5625\n",
      "Iteration: [15200/41391], loss: 139157.15625\n",
      "Iteration: [15300/41391], loss: 294233.28125\n",
      "Iteration: [15400/41391], loss: 355066.375\n",
      "Iteration: [15500/41391], loss: 449510.53125\n",
      "Iteration: [15600/41391], loss: 183792.96875\n",
      "Iteration: [15700/41391], loss: 133378.0625\n",
      "Iteration: [15800/41391], loss: 119573.5\n",
      "Iteration: [15900/41391], loss: 119190.15625\n",
      "Iteration: [16000/41391], loss: 1463502.0\n",
      "Iteration: [16100/41391], loss: 738984.6875\n",
      "Iteration: [16200/41391], loss: 166182.78125\n",
      "Iteration: [16300/41391], loss: 493827.25\n",
      "Iteration: [16400/41391], loss: 188570.078125\n",
      "Iteration: [16500/41391], loss: 403024.96875\n",
      "Iteration: [16600/41391], loss: 287194.5625\n",
      "Iteration: [16700/41391], loss: 306424.375\n",
      "Iteration: [16800/41391], loss: 276855.625\n",
      "Iteration: [16900/41391], loss: 369448.71875\n",
      "Iteration: [17000/41391], loss: 250424.5\n",
      "Iteration: [17100/41391], loss: 320472.65625\n",
      "Iteration: [17200/41391], loss: 376060.9375\n",
      "Iteration: [17300/41391], loss: 152920.546875\n",
      "Iteration: [17400/41391], loss: 382119.1875\n",
      "Iteration: [17500/41391], loss: 225036.6875\n",
      "Iteration: [17600/41391], loss: 147062.71875\n",
      "Iteration: [17700/41391], loss: 117965.3359375\n",
      "Iteration: [17800/41391], loss: 291760.6875\n",
      "Iteration: [17900/41391], loss: 166842.1875\n",
      "Iteration: [18000/41391], loss: 157381.734375\n",
      "Iteration: [18100/41391], loss: 161728.375\n",
      "Iteration: [18200/41391], loss: 200676.625\n",
      "Iteration: [18300/41391], loss: 396802.125\n",
      "Iteration: [18400/41391], loss: 134407.796875\n",
      "Iteration: [18500/41391], loss: 194136.6875\n",
      "Iteration: [18600/41391], loss: 204419.78125\n",
      "Iteration: [18700/41391], loss: 227736.765625\n",
      "Iteration: [18800/41391], loss: 353687.0\n",
      "Iteration: [18900/41391], loss: 196774.4375\n",
      "Iteration: [19000/41391], loss: 460582.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: [19100/41391], loss: 207029.53125\n",
      "Iteration: [19200/41391], loss: 483709.5625\n",
      "Iteration: [19300/41391], loss: 1222838.75\n",
      "Iteration: [19400/41391], loss: 713366.25\n",
      "Iteration: [19500/41391], loss: 183063.0\n",
      "Iteration: [19600/41391], loss: 165635.375\n",
      "Iteration: [19700/41391], loss: 359901.3125\n",
      "Iteration: [19800/41391], loss: 163836.46875\n",
      "Iteration: [19900/41391], loss: 347659.1875\n",
      "Iteration: [20000/41391], loss: 125149.984375\n",
      "Iteration: [20100/41391], loss: 164810.125\n",
      "Iteration: [20200/41391], loss: 170728.28125\n",
      "Iteration: [20300/41391], loss: 377480.25\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CKPT_DIR):\n",
    "    shutil.rmtree(CKPT_DIR)\n",
    "    \n",
    "vgg_weights = np.load(VGG_WEIGHT)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "total_iteration = NUM_EPOCHS * content_iter.n // CONTENT_BATCH_SIZE\n",
    "\n",
    "training_graph = tf.Graph()\n",
    "\n",
    "with training_graph.as_default() as g, tf.Session(config=config, graph=training_graph) as sess:    \n",
    "    style = tf.placeholder(name=\"style\", dtype=tf.float32, \n",
    "                           shape=[STYLE_BATCH_SIZE,STYLE_SIZE,STYLE_SIZE,3])\n",
    "    content = tf.placeholder(name=\"content\", dtype=tf.float32, \n",
    "                             shape=[CONTENT_BATCH_SIZE,CONTENT_SIZE,CONTENT_SIZE,3])\n",
    "\n",
    "    target_style_features = vgg16(style, vgg_weights)\n",
    "    target_content_features = vgg16(content, vgg_weights)\n",
    "    style_gram = tnet(style)\n",
    "    transferred = tnet(content, style_gram)\n",
    "    transferred_features = vgg16(transferred, vgg_weights)\n",
    "    loss = loss_fun(target_style_features, target_content_features, transferred_features)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    summary = tf.summary.FileWriter(graph=g, logdir=LOG_DIR)\n",
    "    style_summary = tf.summary.image(\"style\", style)\n",
    "    content_summary = tf.summary.image(\"content\", content)\n",
    "    transferred_summary = tf.summary.image(\"transferred\", transferred)\n",
    "    image_summary = tf.summary.merge([style_summary, content_summary, transferred_summary])\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
    "\n",
    "    start = time.time()\n",
    "    it = 0\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        content_iter.reset()\n",
    "        style_iter.reset()\n",
    "        for c, _ in content_iter:\n",
    "            it += 1\n",
    "            \n",
    "            if c.shape[0] < CONTENT_BATCH_SIZE:\n",
    "                break\n",
    "                 \n",
    "            try:\n",
    "                s, _ = style_iter.next()\n",
    "            except StopIteration:\n",
    "                style_iter.reset()\n",
    "                s, _ = style_iter.next()\n",
    "                    \n",
    "            _, cur_loss, cur_loss_summary, cur_image_summary = sess.run([train_op, loss, loss_summary, \n",
    "                                                                         image_summary], \n",
    "                                                                        feed_dict={style: s, content: c})\n",
    "            summary.add_summary(cur_loss_summary, it)\n",
    "\n",
    "            if it % LOG_ITER == 0 or it == total_iteration:\n",
    "                print(\"Iteration: [{it}/{num_iter}], loss: {loss}\".format(it=it, num_iter=total_iteration,\n",
    "                                                                          loss=cur_loss))\n",
    "                \n",
    "            if it % SAMPLE_ITER == 0 or it == total_iteration:\n",
    "                summary.add_summary(cur_image_summary, it)\n",
    "                \n",
    "            summary.flush()\n",
    "            \n",
    "        ckpt_path = saver.save(sess, save_path=os.path.join(CKPT_DIR, \"ckpt\"), write_meta_graph=False, \n",
    "                               global_step=it)\n",
    "        print(\"Checkpoint saved as: {ckpt_path}\".format(ckpt_path=ckpt_path))\n",
    "        \n",
    "end = time.time()\n",
    "print(\"Finished {num_iter} in {time} seconds\".format(num_iter=total_iteration, time=end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
