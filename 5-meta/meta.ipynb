{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/holy_arc_64633/.conda/envs/dl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_DIR = \"./styles/\"\n",
    "CONTENT_DIR = \"./train2014/\"\n",
    "VGG_WEIGHT = \"./vgg16_weights.npz\"\n",
    "LOG_DIR = \"./logs\"\n",
    "MODEL_DIR = \"./models\"\n",
    "CKPT_DIR = \"./ckpts\"\n",
    "\n",
    "NUM_EPOCHS = 60\n",
    "LEARNING_RATE = 1e-3\n",
    "CONTENT_BATCH_SIZE = 8\n",
    "STYLE_BATCH_SIZE = 1\n",
    "LOG_ITER = 100\n",
    "SAMPLE_ITER = 100\n",
    "STYLE_SIZE = 256\n",
    "CONTENT_SIZE = 256\n",
    "\n",
    "CONTENT_LOSS_WEIGHT = 1\n",
    "STYLE_LOSS_WEIGHT = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(x, weights):\n",
    "    # substract imagenet mean\n",
    "    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='imagenet_mean')\n",
    "    x = x - mean\n",
    "    \n",
    "    with tf.variable_scope(\"vgg16\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv1_1_W\"]), trainable=False, name='conv1_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv1_1_b\"]), trainable=False, name='conv1_1_b')\n",
    "            conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv1_1 = tf.nn.bias_add(conv1_1, biases)\n",
    "            conv1_1 = tf.nn.relu(conv1_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv1_2_W\"]), trainable=False, name='conv1_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv1_2_b\"]), trainable=False, name='conv1_2_b')\n",
    "            conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv1_2 = tf.nn.bias_add(conv1_2, biases)\n",
    "            conv1_2 = tf.nn.relu(conv1_2, name=scope)\n",
    "\n",
    "        pool1 = tf.nn.avg_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', \n",
    "                               name='pool1')\n",
    "\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv2_1_W\"]), trainable=False, name='conv2_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv2_1_b\"]), trainable=False, name='conv2_1_b')\n",
    "            conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv2_1 = tf.nn.bias_add(conv2_1, biases)\n",
    "            conv2_1 = tf.nn.relu(conv2_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv2_2_W\"]), trainable=False, name='conv2_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv2_2_b\"]), trainable=False, name='conv2_2_b')\n",
    "            conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv2_2 = tf.nn.bias_add(conv2_2, biases)\n",
    "            conv2_2 = tf.nn.relu(conv2_2, name=scope)\n",
    "\n",
    "        pool2 = tf.nn.avg_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_1_W\"]), trainable=False, name='conv3_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_1_b\"]), trainable=False, name='conv3_1_b')\n",
    "            conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_1 = tf.nn.bias_add(conv3_1, biases)\n",
    "            conv3_1 = tf.nn.relu(conv3_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_2_W\"]), trainable=False, name='conv3_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_2_b\"]), trainable=False, name='conv3_2_b')\n",
    "            conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_2 = tf.nn.bias_add(conv3_2, biases)\n",
    "            conv3_2 = tf.nn.relu(conv3_2, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv3_3_W\"]), trainable=False, name='conv3_3_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv3_3_b\"]), trainable=False, name='conv3_3_b')\n",
    "            conv3_3 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv3_3 = tf.nn.bias_add(conv3_3, biases)\n",
    "            conv3_3 = tf.nn.relu(conv3_3, name=scope)\n",
    "\n",
    "        pool3 = tf.nn.avg_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_1_W\"]), trainable=False, name='conv4_1_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_1_b\"]), trainable=False, name='conv4_1_b')\n",
    "            conv4_1 = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_1 = tf.nn.bias_add(conv4_1, biases)\n",
    "            conv4_1 = tf.nn.relu(conv4_1, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_2_W\"]), trainable=False, name='conv4_2_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_2_b\"]), trainable=False, name='conv4_2_b')\n",
    "            conv4_2 = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_2 = tf.nn.bias_add(conv4_2, biases)\n",
    "            conv4_2 = tf.nn.relu(conv4_2, name=scope)\n",
    "\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.get_variable(initializer=tf.constant(weights[\"conv4_3_W\"]), trainable=False, name='conv4_3_W')\n",
    "            biases = tf.get_variable(initializer=tf.constant(weights[\"conv4_3_b\"]), trainable=False, name='conv4_3_b')\n",
    "            conv4_3 = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            conv4_3 = tf.nn.bias_add(conv4_3, biases)\n",
    "            conv4_3 = tf.nn.relu(conv4_3, name=scope)\n",
    "            \n",
    "    return conv1_2, conv2_2, conv3_3, conv4_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_norm(x, name, epsilon=1e-5):\n",
    "    with tf.variable_scope(name):\n",
    "        gamma = tf.get_variable(shape=x.shape[-1], name=\"gamma\")\n",
    "        beta = tf.get_variable(shape=x.shape[-1], name=\"beta\")\n",
    "        mean, var = tf.nn.moments(x, axes=[1,2], keep_dims=True)\n",
    "        x = tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon, name=\"norm\")\n",
    "    return x\n",
    "\n",
    "def conv(x, name, filters, kernel_size, strides, norm=instance_norm, act=tf.nn.relu):\n",
    "    padding = kernel_size//2\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.pad(x, paddings=[[0,0],[padding,padding],[padding,padding],[0,0]], mode=\"REFLECT\")\n",
    "        x = tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size, strides=strides, name=\"conv\")\n",
    "        if norm is not None: x = norm(x, name=\"norm\")\n",
    "        if act is not None: x = act(x, name=\"act\")\n",
    "    return x\n",
    "\n",
    "def fixed_conv(x, name, conv_w, conv_b, strides, norm=instance_norm, act=tf.nn.relu):\n",
    "    padding = conv_w.shape[1]//2\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.pad(x, paddings=[[0,0],[padding,padding],[padding,padding],[0,0]], mode=\"REFLECT\")\n",
    "        x = tf.nn.conv2d(x, conv_w, [1, strides, strides, 1], \"VALID\", name=\"conv\")\n",
    "        x = tf.nn.bias_add(x, conv_b)\n",
    "        if norm is not None: x = norm(x, name=\"norm\")\n",
    "        if act is not None: x = act(x, name=\"act\")\n",
    "    return x\n",
    "\n",
    "def residual_block(x, name, conv1_w, conv1_b, conv2_w, conv2_b):\n",
    "    with tf.variable_scope(name):\n",
    "        residual = x\n",
    "        x = fixed_conv(x, \"conv1\", conv1_w, conv1_b, strides=1)\n",
    "        x = fixed_conv(x, \"conv2\", conv2_w, conv2_b, strides=1, act=None)\n",
    "    return x + residual\n",
    "\n",
    "def upsample(x, name, conv_w, conv_b, strides):\n",
    "    shape = x.shape.as_list()\n",
    "    inferred_shape = tf.shape(x)\n",
    "    w, h = shape[1] or inferred_shape[1], shape[2] or inferred_shape[2]\n",
    "    x = tf.image.resize_images(x, size=[w * strides, h * strides])\n",
    "    x = fixed_conv(x, name, conv_w, conv_b, strides=1)\n",
    "    return x\n",
    "\n",
    "def tnet(x, weights, biases):   \n",
    "    with tf.variable_scope(\"tnet\", reuse=tf.AUTO_REUSE):\n",
    "        conv1 = conv(x, \"conv1\", filters=32, kernel_size=9, strides=1)\n",
    "        conv2 = fixed_conv(conv1, \"conv2\", weights[\"conv2\"], biases[\"conv2\"], strides=2)\n",
    "        conv3 = fixed_conv(conv2, \"conv3\", weights[\"conv3\"], biases[\"conv3\"], strides=2,)\n",
    "        res1 = residual_block(conv3, \"res1\", weights[\"res1_1\"], biases[\"res1_1\"], weights[\"res1_2\"], \n",
    "                              biases[\"res1_2\"])\n",
    "        res2 = residual_block(res1, \"res2\", weights[\"res2_1\"], biases[\"res2_1\"], weights[\"res2_1\"], \n",
    "                              biases[\"res2_1\"])\n",
    "        res3 = residual_block(res2, \"res3\", weights[\"res3_1\"], biases[\"res3_1\"], weights[\"res3_1\"], \n",
    "                              biases[\"res3_1\"])\n",
    "        res4 = residual_block(res3, \"res4\", weights[\"res4_1\"], biases[\"res4_1\"], weights[\"res4_1\"], \n",
    "                              biases[\"res4_1\"])\n",
    "        res5 = residual_block(res4, \"res5\", weights[\"res5_1\"], biases[\"res5_1\"], weights[\"res5_1\"], \n",
    "                              biases[\"res5_1\"])\n",
    "        up1 = upsample(res5, \"up1\", weights[\"up1\"], biases[\"up1\"], strides=2)\n",
    "        up2 = upsample(up1, \"up2\", weights[\"up2\"], biases[\"up2\"], strides=2)\n",
    "        conv4 = conv(up2, \"conv4\", filters=3, kernel_size=9, strides=1, norm=None, act=None)\n",
    "    return tf.clip_by_value(conv4, 0., 255.)\n",
    "\n",
    "def meta(vgg_out):\n",
    "    conv1_2, conv2_2, conv3_3, conv4_3 = vgg_out\n",
    "    conv1_2_mean, conv1_2_var = tf.nn.moments(conv1_2, axes=[1,2])\n",
    "    conv2_2_mean, conv2_2_var = tf.nn.moments(conv2_2, axes=[1,2])\n",
    "    conv3_3_mean, conv3_3_var = tf.nn.moments(conv3_3, axes=[1,2])\n",
    "    conv4_3_mean, conv4_3_var = tf.nn.moments(conv4_3, axes=[1,2])\n",
    "    concat = tf.concat([conv1_2_mean, conv1_2_var, conv2_2_mean, conv2_2_var, conv3_3_mean, conv3_3_var,\n",
    "                        conv4_3_mean, conv4_3_var], axis=1)\n",
    "    dense = tf.layers.dense(concat, units=1792)\n",
    "    split = tf.split(dense, num_or_size_splits=14, axis=1)\n",
    "    \n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    weights[\"conv2\"] = tf.reshape(tf.layers.dense(split[0], units=3 * 3 * 32 * 64), shape=(3, 3, 32, 64))\n",
    "    biases[\"conv2\"] = tf.squeeze(tf.layers.dense(split[0], units=64))\n",
    "    weights[\"conv3\"] = tf.reshape(tf.layers.dense(split[1], units=3 * 3 * 64 * 128), shape=(3, 3, 64, 128))\n",
    "    biases[\"conv3\"] = tf.squeeze(tf.layers.dense(split[1], units=128))\n",
    "    \n",
    "    weights[\"res1_1\"] = tf.reshape(tf.layers.dense(split[2], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res1_1\"] = tf.squeeze(tf.layers.dense(split[2], units=128))\n",
    "    weights[\"res1_2\"] = tf.reshape(tf.layers.dense(split[3], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res1_2\"] = tf.squeeze(tf.layers.dense(split[3], units=128))\n",
    "    weights[\"res2_1\"] = tf.reshape(tf.layers.dense(split[4], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res2_1\"] = tf.squeeze(tf.layers.dense(split[4], units=128))\n",
    "    weights[\"res2_2\"] = tf.reshape(tf.layers.dense(split[5], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res2_2\"] = tf.squeeze(tf.layers.dense(split[5], units=128))\n",
    "    weights[\"res3_1\"] = tf.reshape(tf.layers.dense(split[6], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res3_1\"] = tf.squeeze(tf.layers.dense(split[6], units=128))\n",
    "    weights[\"res3_2\"] = tf.reshape(tf.layers.dense(split[7], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res3_2\"] = tf.squeeze(tf.layers.dense(split[7], units=128))\n",
    "    weights[\"res4_1\"] = tf.reshape(tf.layers.dense(split[8], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res4_1\"] = tf.squeeze(tf.layers.dense(split[8], units=128))\n",
    "    weights[\"res4_2\"] = tf.reshape(tf.layers.dense(split[9], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res4_2\"] = tf.squeeze(tf.layers.dense(split[9], units=128))\n",
    "    weights[\"res5_1\"] = tf.reshape(tf.layers.dense(split[10], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res5_1\"] = tf.squeeze(tf.layers.dense(split[10], units=128))\n",
    "    weights[\"res5_2\"] = tf.reshape(tf.layers.dense(split[11], units=3 * 3 * 128 * 128), shape=(3, 3, 128, 128))\n",
    "    biases[\"res5_2\"] = tf.squeeze(tf.layers.dense(split[11], units=128))\n",
    "    \n",
    "    weights[\"up1\"] = tf.reshape(tf.layers.dense(split[12], units=3 * 3 * 128 * 64), shape=(3, 3, 128, 64))\n",
    "    biases[\"up1\"] = tf.squeeze(tf.layers.dense(split[12], units=64))\n",
    "    weights[\"up2\"] = tf.reshape(tf.layers.dense(split[13], units=3 * 3 * 64 * 32), shape=(3, 3, 64, 32))\n",
    "    biases[\"up2\"] = tf.squeeze(tf.layers.dense(split[13], units=32))\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    batch_size, w, h, ch = x.shape.as_list()\n",
    "    x = tf.reshape(x, [batch_size, w * h, ch])\n",
    "    return tf.matmul(x, x, transpose_a=True) / (ch * w * h)\n",
    "\n",
    "def loss_fun(target_style_features, target_content_features, transferred_features, transferred,\n",
    "             style_loss_weight=STYLE_LOSS_WEIGHT, content_loss_weight=CONTENT_LOSS_WEIGHT):\n",
    "    content_loss = tf.reduce_mean(tf.subtract(transferred_features[1], target_content_features[1]) ** 2, \n",
    "                                  [1, 2, 3])  \n",
    "\n",
    "    style_loss = 0\n",
    "    for i in range(len(transferred_features)):\n",
    "        gram_target = gram_matrix(target_style_features[i])\n",
    "        gram_transferred = gram_matrix(transferred_features[i])\n",
    "        style_loss += tf.reduce_mean(tf.subtract(gram_target, gram_transferred) ** 2, [1, 2])  \n",
    "        \n",
    "    return tf.reduce_mean(content_loss_weight * content_loss \n",
    "                          + style_loss_weight * style_loss\n",
    "                          + 1e-5 * tf.image.total_variation(transferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82783 images belonging to 1 classes.\n",
      "Found 21 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "iterator = tf.keras.preprocessing.image.DirectoryIterator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "content_iter = iterator(directory=CONTENT_DIR, batch_size=CONTENT_BATCH_SIZE, \n",
    "                        target_size=(CONTENT_SIZE,CONTENT_SIZE), image_data_generator=datagen, shuffle=True)\n",
    "style_iter = iterator(directory=STYLE_DIR, batch_size=STYLE_BATCH_SIZE,\n",
    "                      target_size=(STYLE_SIZE,STYLE_SIZE), image_data_generator=datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: [100/620872], loss: 203612288.0\n",
      "Iteration: [200/620872], loss: 37005680.0\n",
      "Iteration: [300/620872], loss: 88067048.0\n",
      "Iteration: [400/620872], loss: 156271200.0\n",
      "Iteration: [500/620872], loss: 106962256.0\n",
      "Iteration: [600/620872], loss: 37562332.0\n",
      "Iteration: [700/620872], loss: 104418896.0\n",
      "Iteration: [800/620872], loss: 58532180.0\n",
      "Iteration: [900/620872], loss: 30773192.0\n",
      "Iteration: [1000/620872], loss: 55904152.0\n",
      "Iteration: [1100/620872], loss: 7077652.0\n",
      "Iteration: [1200/620872], loss: 12791204.0\n",
      "Iteration: [1300/620872], loss: 34451856.0\n",
      "Iteration: [1400/620872], loss: 41571176.0\n",
      "Iteration: [1500/620872], loss: 53679216.0\n",
      "Iteration: [1600/620872], loss: 9196070.0\n",
      "Iteration: [1700/620872], loss: 43755056.0\n",
      "Iteration: [1800/620872], loss: 9651416.0\n",
      "Iteration: [1900/620872], loss: 17144158.0\n",
      "Iteration: [2000/620872], loss: 22871446.0\n",
      "Iteration: [2100/620872], loss: 22925834.0\n",
      "Iteration: [2200/620872], loss: 17135156.0\n",
      "Iteration: [2300/620872], loss: 14873421.0\n",
      "Iteration: [2400/620872], loss: 28995560.0\n",
      "Iteration: [2500/620872], loss: 5009484.5\n",
      "Iteration: [2600/620872], loss: 11028480.0\n",
      "Iteration: [2700/620872], loss: 29779268.0\n",
      "Iteration: [2800/620872], loss: 8647790.0\n",
      "Iteration: [2900/620872], loss: 5106382.0\n",
      "Iteration: [3000/620872], loss: 28363148.0\n",
      "Iteration: [3100/620872], loss: 6872596.0\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CKPT_DIR):\n",
    "    shutil.rmtree(CKPT_DIR)\n",
    "    \n",
    "vgg_weights = np.load(VGG_WEIGHT)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "total_iteration = NUM_EPOCHS * content_iter.n // CONTENT_BATCH_SIZE\n",
    "\n",
    "training_graph = tf.Graph()\n",
    "\n",
    "with training_graph.as_default() as g, tf.Session(config=config, graph=training_graph) as sess:    \n",
    "    style = tf.placeholder(name=\"style\", dtype=tf.float32, \n",
    "                           shape=[STYLE_BATCH_SIZE,STYLE_SIZE,STYLE_SIZE,3])\n",
    "    content = tf.placeholder(name=\"content\", dtype=tf.float32, \n",
    "                             shape=[CONTENT_BATCH_SIZE,CONTENT_SIZE,CONTENT_SIZE,3])\n",
    "\n",
    "    target_style_features = vgg16(style, vgg_weights)\n",
    "    target_content_features = vgg16(content, vgg_weights)\n",
    "    \n",
    "    vgg_out = vgg16(style, vgg_weights)\n",
    "    weights, biases = meta(vgg_out)\n",
    "    transferred = tnet(content, weights, biases)\n",
    "    transferred_features = vgg16(transferred, vgg_weights)\n",
    "    loss = loss_fun(target_style_features, target_content_features, transferred_features, transferred)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    summary = tf.summary.FileWriter(graph=g, logdir=LOG_DIR)\n",
    "    style_summary = tf.summary.image(\"style\", style)\n",
    "    content_summary = tf.summary.image(\"content\", content)\n",
    "    transferred_summary = tf.summary.image(\"transferred\", transferred)\n",
    "    image_summary = tf.summary.merge([style_summary, content_summary, transferred_summary])\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
    "\n",
    "    start = time.time()\n",
    "    it = 0\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        content_iter.reset()\n",
    "        style_iter.reset()\n",
    "        for c, _ in content_iter:\n",
    "            it += 1\n",
    "            \n",
    "            if c.shape[0] < CONTENT_BATCH_SIZE:\n",
    "                break\n",
    "                 \n",
    "            try:\n",
    "                s, _ = style_iter.next()\n",
    "            except StopIteration:\n",
    "                style_iter.reset()\n",
    "                s, _ = style_iter.next()\n",
    "                    \n",
    "            _, cur_loss, cur_loss_summary, cur_image_summary = sess.run([train_op, loss, loss_summary, \n",
    "                                                                         image_summary], \n",
    "                                                                        feed_dict={style: s, content: c})\n",
    "            summary.add_summary(cur_loss_summary, it)\n",
    "\n",
    "            if it % LOG_ITER == 0 or it == total_iteration:\n",
    "                print(\"Iteration: [{it}/{num_iter}], loss: {loss}\".format(it=it, num_iter=total_iteration,\n",
    "                                                                          loss=cur_loss))\n",
    "                \n",
    "            if it % SAMPLE_ITER == 0 or it == total_iteration:\n",
    "                summary.add_summary(cur_image_summary, it)\n",
    "                \n",
    "            summary.flush()\n",
    "            \n",
    "        ckpt_path = saver.save(sess, save_path=os.path.join(CKPT_DIR, \"ckpt\"), write_meta_graph=False, \n",
    "                               global_step=it)\n",
    "        print(\"Checkpoint saved as: {ckpt_path}\".format(ckpt_path=ckpt_path))\n",
    "        \n",
    "end = time.time()\n",
    "print(\"Finished {num_iter} in {time} seconds\".format(num_iter=total_iteration, time=end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
